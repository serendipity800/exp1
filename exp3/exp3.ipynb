{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ab9dbafc-c62e-4401-a1a1-0118ab8f61c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "# 设置随机种子，以便结果可以复现\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 定义文本字段和标签字段\n",
    "TEXT = data.Field(tokenize='spacy', tokenizer_language='en_core_web_sm')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "# 加载IMDB数据集\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# 构建词汇表，并用预训练的词向量初始化\n",
    "TEXT.build_vocab(train_data, max_size=25000, vectors=\"glove.6B.100d\", unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "# 创建迭代器\n",
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b10e8a30-4059-4b18-827c-ddaaa7f6ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(hidden.squeeze(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37b92dbc-7b28-4c3d-bb67-dbb97bdbb701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:33<00:00,  4.19it/s]\n",
      "100%|██████████| 391/391 [01:32<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tLSTM Model:\n",
      "\t\tTrain Loss: 0.684 | Train Acc: 55.13%\n",
      "\t\tVal. Loss: 0.693 |  Val. Acc: 58.22%\n",
      "\t\tVal Acc on class 0: 0.44 |  Val. Acc on class 1: 0.71%\n",
      "\tGRU Model:\n",
      "\t\tTrain Loss: 0.697 | Train Acc: 52.77%\n",
      "\t\tVal. Loss: 0.679 |  Val. Acc: 57.20%\n",
      "\t\tVal Acc on class 0: 0.86 |  Val. Acc on class 1: 0.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:33<00:00,  4.19it/s]\n",
      "100%|██████████| 391/391 [01:31<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tLSTM Model:\n",
      "\t\tTrain Loss: 0.690 | Train Acc: 53.13%\n",
      "\t\tVal. Loss: 0.658 |  Val. Acc: 61.35%\n",
      "\t\tVal Acc on class 0: 0.88 |  Val. Acc on class 1: 0.33%\n",
      "\tGRU Model:\n",
      "\t\tTrain Loss: 0.608 | Train Acc: 66.01%\n",
      "\t\tVal. Loss: 0.504 |  Val. Acc: 76.62%\n",
      "\t\tVal Acc on class 0: 0.90 |  Val. Acc on class 1: 0.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:33<00:00,  4.20it/s]\n",
      "100%|██████████| 391/391 [01:31<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tLSTM Model:\n",
      "\t\tTrain Loss: 0.675 | Train Acc: 57.61%\n",
      "\t\tVal. Loss: 0.713 |  Val. Acc: 56.81%\n",
      "\t\tVal Acc on class 0: 0.96 |  Val. Acc on class 1: 0.17%\n",
      "\tGRU Model:\n",
      "\t\tTrain Loss: 0.415 | Train Acc: 80.89%\n",
      "\t\tVal. Loss: 0.335 |  Val. Acc: 86.11%\n",
      "\t\tVal Acc on class 0: 0.85 |  Val. Acc on class 1: 0.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:33<00:00,  4.20it/s]\n",
      "100%|██████████| 391/391 [01:33<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tLSTM Model:\n",
      "\t\tTrain Loss: 0.586 | Train Acc: 68.64%\n",
      "\t\tVal. Loss: 0.432 |  Val. Acc: 79.95%\n",
      "\t\tVal Acc on class 0: 0.85 |  Val. Acc on class 1: 0.72%\n",
      "\tGRU Model:\n",
      "\t\tTrain Loss: 0.329 | Train Acc: 86.00%\n",
      "\t\tVal. Loss: 0.319 |  Val. Acc: 87.09%\n",
      "\t\tVal Acc on class 0: 0.84 |  Val. Acc on class 1: 0.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:33<00:00,  4.20it/s]\n",
      "100%|██████████| 391/391 [01:31<00:00,  4.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tLSTM Model:\n",
      "\t\tTrain Loss: 0.388 | Train Acc: 83.03%\n",
      "\t\tVal. Loss: 0.348 |  Val. Acc: 85.45%\n",
      "\t\tVal Acc on class 0: 0.81 |  Val. Acc on class 1: 0.88%\n",
      "\tGRU Model:\n",
      "\t\tTrain Loss: 0.276 | Train Acc: 88.64%\n",
      "\t\tVal. Loss: 0.325 |  Val. Acc: 86.93%\n",
      "\t\tVal Acc on class 0: 0.93 |  Val. Acc on class 1: 0.79%\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "writer = SummaryWriter(log_dir = \"/root/tf-logs/text\")\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super(GRUModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        \n",
    "        # Determine the input size to the fully connected layer based on GRU configuration\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # text shape: [seq_len, batch_size]\n",
    "        embedded = self.embedding(text)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # embedded shape: [seq_len, batch_size, embedding_dim]\n",
    "        output, hidden = self.gru(embedded)\n",
    "\n",
    "        # output shape: [seq_len, batch_size, hidden_dim * num_directions]\n",
    "        # hidden shape: [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        \n",
    "        # Select the last hidden state as the final output (many-to-one architecture)\n",
    "        if self.gru.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "\n",
    "        # hidden shape: [batch_size, hidden_dim * num_directions]\n",
    "\n",
    "        # Apply fully connected layer\n",
    "        output = self.fc(hidden)\n",
    "\n",
    "        # output shape: [batch_size, output_dim]\n",
    "        return output\n",
    "        \n",
    "lstm_train_step = 0\n",
    "gru_train_step = 0\n",
    "lstm_val_step = 0\n",
    "gru_val_step = 0\n",
    "\n",
    "def train_lstm(model, iterator, optimizer, criterion):\n",
    "    global lstm_train_step\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc, acc_class0, acc_class1 = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('train_loss_lstm', loss.item(), lstm_train_step)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        lstm_train_step += 1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def train_gru(model, iterator, optimizer, criterion):\n",
    "    global gru_train_step\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    for batch in tqdm(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc, acc_class0, acc_class1 = binary_accuracy(predictions, batch.label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        writer.add_scalar('train_loss_gru', loss.item(), gru_train_step)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        gru_train_step += 1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_lstm(model, iterator, criterion):\n",
    "    global lstm_val_step\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_acc_class0, epoch_acc_class1 = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc, acc_class0, acc_class1 = binary_accuracy(predictions, batch.label)\n",
    "            writer.add_scalar('val_loss_lstm', loss.item(), lstm_val_step)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_acc_class0 += acc_class0\n",
    "            epoch_acc_class1 += acc_class1\n",
    "            lstm_val_step += 1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_acc_class0 / len(iterator), epoch_acc_class1 / len(iterator)\n",
    "\n",
    "def evaluate_gru(model, iterator, criterion):\n",
    "    global gru_val_step\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_acc_class0, epoch_acc_class1 = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            acc, acc_class0, acc_class1 = binary_accuracy(predictions, batch.label)\n",
    "            writer.add_scalar('val_loss_gru', loss.item(), gru_val_step)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_acc_class0 += acc_class0\n",
    "            epoch_acc_class1 += acc_class1\n",
    "            gru_val_step += 1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_acc_class0 / len(iterator), epoch_acc_class1 / len(iterator)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    acc_class0 = torch.mean(correct[y == 0]) if (y == 0).sum() > 0 else 0.0  # Accuracy for class 0\n",
    "    acc_class1 = torch.mean(correct[y == 1]) if (y == 1).sum() > 0 else 0.0  # Accuracy for class 1\n",
    "\n",
    "    return acc, acc_class0, acc_class1\n",
    "\n",
    "# 设置模型参数\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "NUM_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# 初始化模型和优化器\n",
    "lstm_model = LSTMModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "gru_model = GRUModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT)\n",
    "\n",
    "optimizer_lstm = optim.Adam(lstm_model.parameters())\n",
    "optimizer_gru = optim.Adam(gru_model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# 将模型和数据移到GPU（如果可用）\n",
    "lstm_model = lstm_model.to(device)\n",
    "gru_model = gru_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# 训练模型\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss_lstm, train_acc_lstm = train_lstm(lstm_model, train_iterator, optimizer_lstm, criterion)\n",
    "    train_loss_gru, train_acc_gru = train_gru(gru_model, train_iterator, optimizer_gru, criterion)\n",
    "    valid_loss_lstm, valid_acc_lstm, valid_acc_class0_lstm, valid_acc_class1_lstm = evaluate_lstm(lstm_model, test_iterator, criterion)\n",
    "    valid_loss_gru, valid_acc_gru, valid_acc_class0_gru, valid_acc_class1_gru = evaluate_gru(gru_model, test_iterator, criterion)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tLSTM Model:')\n",
    "    print(f'\\t\\tTrain Loss: {train_loss_lstm:.3f} | Train Acc: {train_acc_lstm*100:.2f}%')\n",
    "    print(f'\\t\\tVal. Loss: {valid_loss_lstm:.3f} |  Val. Acc: {valid_acc_lstm*100:.2f}%')\n",
    "    print(f'\\t\\tVal Acc on class 0: {valid_acc_class0_lstm:.2f} |  Val. Acc on class 1: {valid_acc_class1_lstm:.2f}%')\n",
    "    print(f'\\tGRU Model:')\n",
    "    print(f'\\t\\tTrain Loss: {train_loss_gru:.3f} | Train Acc: {train_acc_gru*100:.2f}%')\n",
    "    print(f'\\t\\tVal. Loss: {valid_loss_gru:.3f} |  Val. Acc: {valid_acc_gru*100:.2f}%')\n",
    "    print(f'\\t\\tVal Acc on class 0: {valid_acc_class0_gru:.2f} |  Val. Acc on class 1: {valid_acc_class1_gru:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c5c185-e7d7-4179-97eb-ad484228cdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
